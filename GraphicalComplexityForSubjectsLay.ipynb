{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8eWnolLUqeBCUaaHPtyCi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTNXUQaKQonl","executionInfo":{"status":"ok","timestamp":1750797521315,"user_tz":240,"elapsed":329680,"user":{"displayName":"Vidya S.","userId":"08122589015459655768"}},"outputId":"1208c3da-1055-459a-8ac1-2b2682c21687"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Weak difference in NetworkNodes for ('physicalSciences', 'biologicalSciences'): t = 1.795, p = 0.0765\n","Significant difference in NetworkEdges for ('physicalSciences', 'biologicalSciences'): t = 2.558, p = 0.0125\n","Significant difference in Network Degree for ('physicalSciences', 'biologicalSciences'): t = 2.208, p = 0.0302\n","No significant difference in NetworkDensity for ('physicalSciences', 'biologicalSciences'): p = 0.9591, t = 0.051\n","No significant difference in NetworkAvgClustering for ('physicalSciences', 'biologicalSciences'): p = 0.1703, t = 1.384\n","No significant difference in NetworkLargestComponent for ('physicalSciences', 'biologicalSciences'): p = 0.4157, t = 0.818\n","Significant difference in NetworkDegreeWeighted for ('physicalSciences', 'biologicalSciences'): t = 2.336, p = 0.0221\n","No significant difference in NetworkNodes for ('physicalSciences', 'socialSciences'): p = 0.2622, t = 1.131\n","No significant difference in NetworkEdges for ('physicalSciences', 'socialSciences'): p = 0.3273, t = 0.987\n","No significant difference in Network Degree for ('physicalSciences', 'socialSciences'): p = 0.4345, t = 0.786\n","No significant difference in NetworkDensity for ('physicalSciences', 'socialSciences'): p = 0.4891, t = -0.696\n","No significant difference in NetworkAvgClustering for ('physicalSciences', 'socialSciences'): p = 0.6538, t = 0.451\n","No significant difference in NetworkLargestComponent for ('physicalSciences', 'socialSciences'): p = 0.2861, t = 1.075\n","No significant difference in NetworkDegreeWeighted for ('physicalSciences', 'socialSciences'): p = 0.3710, t = 0.901\n","No significant difference in NetworkNodes for ('biologicalSciences', 'socialSciences'): p = 0.9381, t = -0.078\n","No significant difference in NetworkEdges for ('biologicalSciences', 'socialSciences'): p = 0.4438, t = -0.770\n","No significant difference in Network Degree for ('biologicalSciences', 'socialSciences'): p = 0.3410, t = -0.959\n","No significant difference in NetworkDensity for ('biologicalSciences', 'socialSciences'): p = 0.3370, t = -0.967\n","No significant difference in NetworkAvgClustering for ('biologicalSciences', 'socialSciences'): p = 0.3397, t = -0.962\n","No significant difference in NetworkLargestComponent for ('biologicalSciences', 'socialSciences'): p = 0.3755, t = 0.892\n","No significant difference in NetworkDegreeWeighted for ('biologicalSciences', 'socialSciences'): p = 0.2908, t = -1.065\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","import itertools\n","from scipy.stats import ttest_rel\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","import math\n","import pandas as pd\n","import networkx as nx\n","Subjects = {}\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/14-Indices (2).txt\"\n","file_path2 = '/content/drive/My Drive/Colab Notebooks/FinalizedSummaries.csv'\n","df = pd.read_csv(file_path2)\n","df = df.iloc[:300]\n","\n","NetworkNodes = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","NetworkEdges = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","NetworkDegree = {\"Lay Summary\" : {}, \"Technical Summary\": {}}\n","NetworkDensity = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","NetworkAvgClustering = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","NetworkLargestComponent = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","NetworkDegreeWeighted = {\"Lay Summary\": {}, \"Technical Summary\": {}}\n","\n","\n","graph_measures = [\n","    NetworkNodes,\n","    NetworkEdges,\n","    NetworkDegree,\n","    NetworkDensity,\n","    NetworkAvgClustering,\n","    NetworkLargestComponent,\n","    NetworkDegreeWeighted\n","]\n","\n","graph_measure_names = [\n","    \"NetworkNodes\",\n","    \"NetworkEdges\",\n","    \"Network Degree\",\n","    \"NetworkDensity\",\n","    \"NetworkAvgClustering\",\n","    \"NetworkLargestComponent\",\n","    \"NetworkDegreeWeighted\"\n","]\n","\n","\n","def build_pmi_graph(pmi_scores, all_nodes):\n","    G = nx.Graph()\n","    G.add_nodes_from(all_nodes)\n","    for (w1, w2), weight in pmi_scores.items():\n","        G.add_edge(w1, w2, weight=weight)\n","    return G\n","\n","def analyze_graph(G, words):\n","\n","    connected_nodes = [node for node, degree in G.degree()if degree > 0]\n","\n","    return {\n","        'nodes per words': len(connected_nodes) / words,\n","        'edges': int(G.number_of_edges()),\n","        'avg_degree' : sum(dict(G.degree()).values()) / G.number_of_nodes(),\n","        'density': nx.density(G.subgraph(connected_nodes)),\n","        'average clustering': nx.average_clustering(G.subgraph(connected_nodes), weight='weight'),\n","        'largest_component_size':len(max(nx.connected_components(G), key=len)) / G.number_of_nodes(),\n","        'avg_degree_weighted': sum(dict(G.subgraph(connected_nodes).degree(weight='weight')).values()) / len(connected_nodes)\n","\n","    }\n","\n","def findNetworkComplexity(Column):\n","\n","    count = 0\n","\n","    for summary in df[Column]:\n","        count += 1\n","        wordsDict = {}\n","\n","        wordsList = [word.lower() for word in re.findall(r'\\b[a-zA-Z]+\\b', summary)]\n","        sentences = re.split(r'(?<=[.!?])\\s+', summary)\n","\n","        for word in wordsList:\n","            if word in wordsDict:\n","                wordsDict[word] += 1\n","            else:\n","                wordsDict[word] = 1\n","\n","        keys = list(wordsDict.keys())\n","        pairsDict = {pair: 0 for pair in itertools.combinations(keys, 2)}\n","\n","        for pair in pairsDict:\n","            w1, w2 = pair\n","            w1Freq = 0\n","            w2Freq = 0\n","            mutualOccurences = 0\n","            numSentences = len(sentences)\n","\n","            for word in wordsList:\n","              if word == w1:\n","                w1Freq += 1\n","              elif word == w2:\n","                w2Freq += 1\n","\n","            for sentence in sentences:\n","                if w1 in sentence and w2 in sentence:\n","                  mutualOccurences += 1\n","\n","            pW1 = w1Freq / len(wordsList)\n","            pW2 = w2Freq / len(wordsList)\n","            pW1W2 = mutualOccurences / numSentences\n","\n","            if pW1W2 > 0:\n","                pairsDict[pair] = math.log2(pW1W2 / (pW1 * pW2))\n","\n","        refinedPairsDict = {}\n","\n","        for pair in pairsDict:\n","\n","          if pairsDict[pair] != 0:\n","\n","            refinedPairsDict[pair] = pairsDict[pair]\n","\n","        graphMetrics = analyze_graph(build_pmi_graph(refinedPairsDict, set(wordsList)), len(wordsList))\n","\n","        NetworkNodes[Column][count] = graphMetrics['nodes per words']\n","        NetworkEdges[Column][count] = graphMetrics['edges']\n","        NetworkDegree[Column][count] = graphMetrics['avg_degree']\n","        NetworkDensity[Column][count] = graphMetrics['density']\n","        NetworkAvgClustering[Column][count] = graphMetrics['average clustering']\n","        NetworkLargestComponent[Column][count] = graphMetrics['largest_component_size']\n","        NetworkDegreeWeighted[Column][count] = graphMetrics['avg_degree_weighted']\n","\n","\n","    graph_measures = [\n","    NetworkNodes,\n","    NetworkEdges,\n","    NetworkDegree,\n","    NetworkDensity,\n","    NetworkAvgClustering,\n","    NetworkLargestComponent,\n","    NetworkDegreeWeighted\n","\n","    ]\n","\n","\n","findNetworkComplexity(\"Lay Summary\")\n","findNetworkComplexity(\"Technical Summary\")\n","\n","#print(graph_measures)\n","\n","\n","lineNum = 0\n","laySummaryCount = 0\n","techSummaryCount = 0\n","subjectLabels = {}\n","\n","with open(file_path, \"r\") as file:\n","\n","    for line in file:\n","\n","        #Document all the subjects of the summaries\n","        lineNum += 1\n","        if lineNum % 3 == 1:\n","          subject = line.strip().lower()\n","          if subject in Subjects.keys():\n","            Subjects[subject] += 1\n","          else:\n","            Subjects[subject] = 1\n","        else:\n","          # Remove trailing newline and whitespace\n","          preValues = [val.strip() for val in line.strip().split(\",\")]\n","          values = []\n","          for value in preValues:\n","            values.append(float(value))\n","\n","          if lineNum % 3 == 2:\n","\n","            laySummaryCount += 1\n","            subjectLabels[laySummaryCount] = subject.strip().lower()\n","\n","          else:\n","\n","            techSummaryCount += 1\n","\n","\n","physicalSciences = ['environmental sciences', 'earth, atmospheric, and planetary sciences', 'applied physical sciences', 'computer sciences','applied mathematics','engineering','chemistry','statistics','sustainability science','physics','astronomy']\n","biologicalSciences = ['biophysics and computational biology','immunology and inflammation','neuroscience', 'genetics', 'ecology', 'medical sciences','evolution','plant biology','agricultural sciences','applied biological sciences','biochemistry','developmental biology','systems biology','microbiology','pharmacology','cell biology','physiology','population biology']\n","socialSciences = ['economic sciences','psychological and cognitive sciences','demography','political sciences','social sciences','anthropology','']\n","\n","umbrellas = {\n","    'physicalSciences': physicalSciences,\n","    'biologicalSciences': biologicalSciences,\n","    'socialSciences': socialSciences\n","}\n","\n","MasterDictionary3 = {'NetworkNodes' : NetworkNodes, 'NetworkEdges' : NetworkEdges,'NetworkDegree' : NetworkDegree, 'NetworkDensity' : NetworkDensity, 'NetworkAvgClustering' : NetworkAvgClustering, 'NetworkLargestComponent' : NetworkLargestComponent, 'NetworkDegreeWeighted' : NetworkDegreeWeighted }\n","\n","bioQ1Q2Mean = {\"NetworkNodes\":{'q1':0,'q2':0}, 'NetworkEdges': {'q1':0,'q2':0}, 'NetworkDegree' : {'q1':0,'q2':0}, 'NetworkDensity' : {'q1':0,'q2':0}, 'NetworkAvgClustering' : {'q1':0,'q2':0}, 'NetworkLargestComponent' : {'q1':0,'q2':0}, 'NetworkDegreeWeighted': {'q1':0,'q2':0}}\n","physQ1Q2Mean = {\"NetworkNodes\":{'q1':0,'q2':0}, 'NetworkEdges': {'q1':0,'q2':0}, 'NetworkDegree' : {'q1':0,'q2':0}, 'NetworkDensity' : {'q1':0,'q2':0}, 'NetworkAvgClustering' : {'q1':0,'q2':0}, 'NetworkLargestComponent' : {'q1':0,'q2':0}, 'NetworkDegreeWeighted': {'q1':0,'q2':0}}\n","sociQ1Q2Mean = {\"NetworkNodes\":{'q1':0,'q2':0}, 'NetworkEdges': {'q1':0,'q2':0}, 'NetworkDegree' : {'q1':0,'q2':0}, 'NetworkDensity' : {'q1':0,'q2':0}, 'NetworkAvgClustering' : {'q1':0,'q2':0}, 'NetworkLargestComponent' : {'q1':0,'q2':0}, 'NetworkDegreeWeighted': {'q1':0,'q2':0}}\n","\n","for key in MasterDictionary3.keys():\n","  bio = []\n","  phys = []\n","  soci = []\n","\n","  for i in range(1,301):\n","\n","    if subjectLabels[i] in physicalSciences:\n","      phys.append(MasterDictionary3[key][\"Technical Summary\"][i])\n","    elif subjectLabels[i] in biologicalSciences:\n","      bio.append(MasterDictionary3[key][\"Technical Summary\"][i])\n","    elif subjectLabels[i] in socialSciences:\n","      soci.append(MasterDictionary3[key][\"Technical Summary\"][i])\n","  #print(key)\n","  #print(bio)\n","  #print(phys)\n","  #print(soci)\n","\n","  bioQ1Q2Mean[key]['q1'] = np.percentile(np.fromiter(bio, dtype=float),25)\n","  bioQ1Q2Mean[key]['q2'] = np.percentile(np.fromiter(bio, dtype=float),75)\n","  bioQ1Q2Mean[key]['mean'] = np.median(np.fromiter(bio,dtype=float))\n","\n","  physQ1Q2Mean[key]['q1'] = np.percentile(np.fromiter(phys, dtype=float),25)\n","  physQ1Q2Mean[key]['q2'] = np.percentile(np.fromiter(phys, dtype=float),75)\n","  physQ1Q2Mean[key]['mean'] = np.median(np.fromiter(phys,dtype=float))\n","\n","  sociQ1Q2Mean[key]['q1'] = np.percentile(np.fromiter(soci, dtype=float),25)\n","  sociQ1Q2Mean[key]['q2'] = np.percentile(np.fromiter(soci, dtype=float),75)\n","  sociQ1Q2Mean[key]['mean'] = np.median(np.fromiter(soci,dtype=float))\n","\n","results = {}\n","\n","for pair in itertools.combinations(umbrellas.keys(), 2):\n","    results[pair] = {}\n","\n","    subjOne, subjTwo = pair\n","\n","    for name, measure in zip(graph_measure_names, graph_measures):\n","        subOneValues = []\n","        subTwoValues = []\n","\n","        for i in range(1, len(subjectLabels) + 1):\n","            subject = subjectLabels.get(i, \"\").lower()\n","            if subject in umbrellas[subjOne]:\n","                if i in measure[\"Lay Summary\"]:\n","                    subOneValues.append(measure[\"Lay Summary\"][i])\n","            elif subject in umbrellas[subjTwo]:\n","                if i in measure[\"Lay Summary\"]:\n","                    subTwoValues.append(measure[\"Lay Summary\"][i])\n","\n","        # Make lengths equal for paired test\n","        desiredLength = min(len(subOneValues), len(subTwoValues))\n","        subOneValues = subOneValues[:desiredLength]\n","        subTwoValues = subTwoValues[:desiredLength]\n","\n","        if desiredLength > 0:\n","            t_stat, p_val = ttest_rel(subOneValues, subTwoValues)\n","            results[pair][name] = {'t': t_stat, 'p': p_val}\n","        else:\n","            results[pair][name] = {'t': np.nan, 'p': np.nan}\n","\n","# Print significant results\n","for pair, metrics in results.items():\n","    for metric_name, stats in metrics.items():\n","        if np.isnan(stats['p']):\n","            print(f\"No data for {metric_name} in comparison {pair}\")\n","        elif stats['p'] < 0.05:\n","            print(f\"Significant difference in {metric_name} for {pair}: t = {stats['t']:.3f}, p = {stats['p']:.4f}\")\n","        elif stats['p'] < 0.1:\n","            print(f\"Weak difference in {metric_name} for {pair}: t = {stats['t']:.3f}, p = {stats['p']:.4f}\")\n","        else:\n","            print(f\"No significant difference in {metric_name} for {pair}: p = {stats['p']:.4f}, t = {stats['t']:.3f}\")\n","\n","\n","#print(bioQ1Q2Mean)\n","#print(physQ1Q2Mean)\n","#print(sociQ1Q2Mean)"]}]}